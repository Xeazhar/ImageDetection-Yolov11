{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e847c1d-1495-4e96-9a38-dae19e3a6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3cf32-dbe0-4f4e-a351-b7e86ff88e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell Jupyter snippet for generating stratified K-folds from nested dataset\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pathlib import Path\n",
    "\n",
    "# Parameters: adjust these paths as needed\n",
    "dataset_dir = r\"C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\"\n",
    "output_dir = r\"C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\"\n",
    "n_splits = 5\n",
    "random_state = 42\n",
    "copy_files = True  # Set to False to use symlinks instead of copying\n",
    "\n",
    "# Gather all samples and class labels\n",
    "train_dir = Path(dataset_dir) / 'train'\n",
    "test_dir = Path(dataset_dir) / 'test'\n",
    "samples = []  # (img_path, lbl_path, class_name)\n",
    "class_names = []\n",
    "for cls_dir in sorted(train_dir.iterdir()):\n",
    "    if not cls_dir.is_dir():\n",
    "        continue\n",
    "    cls = cls_dir.name\n",
    "    class_names.append(cls)\n",
    "    for img_path in (cls_dir / 'images').glob('*.*'):\n",
    "        lbl_path = cls_dir / 'labels' / f\"{img_path.stem}.txt\"\n",
    "        samples.append((img_path, lbl_path, cls))\n",
    "\n",
    "# Prepare arrays for stratification\n",
    "image_paths = [s[0] for s in samples]\n",
    "label_paths = [s[1] for s in samples]\n",
    "y = [class_names.index(s[2]) for s in samples]\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "summary = {}\n",
    "\n",
    "# Create folds\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(image_paths, y), start=1):\n",
    "    fold_base = Path(output_dir) / f\"fold_{fold_idx}\"\n",
    "    summary[fold_idx] = {'train': len(train_idx), 'val': len(val_idx), 'test': 0}\n",
    "\n",
    "    # Train/val splits\n",
    "    for split, idxs in [('train', train_idx), ('val', val_idx)]:\n",
    "        for cls in class_names:\n",
    "            (fold_base / split / cls / 'images').mkdir(parents=True, exist_ok=True)\n",
    "            (fold_base / split / cls / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "        for i in idxs:\n",
    "            img_src, lbl_src, cls_name = image_paths[i], label_paths[i], samples[i][2]\n",
    "            img_dst = fold_base / split / cls_name / 'images' / img_src.name\n",
    "            lbl_dst = fold_base / split / cls_name / 'labels' / lbl_src.name\n",
    "            if copy_files:\n",
    "                shutil.copy(img_src, img_dst)\n",
    "                shutil.copy(lbl_src, lbl_dst)\n",
    "            else:\n",
    "                os.symlink(img_src.resolve(), img_dst)\n",
    "                os.symlink(lbl_src.resolve(), lbl_dst)\n",
    "\n",
    "    # Copy test set unchanged\n",
    "    if test_dir.exists():\n",
    "        dst_test = fold_base / 'test'\n",
    "        if dst_test.exists():\n",
    "            shutil.rmtree(dst_test)\n",
    "        shutil.copytree(test_dir, dst_test)\n",
    "        test_images_dir = dst_test / 'images'\n",
    "        summary[fold_idx]['test'] = len(list(test_images_dir.glob('*.*')))\n",
    "\n",
    "# Print out summary to verify\n",
    "print(\"K-fold generation summary:\")\n",
    "for fold, counts in summary.items():\n",
    "    print(f\"Fold {fold}: train={counts['train']}, val={counts['val']}, test={counts['test']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a2f968-455e-418f-9c8c-677ca0dffe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "=== Training Fold 1/5 on folder C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1 ===\n",
      "Ultralytics 8.3.133  Python-3.9.21 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.2, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=models\\yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=results, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=5, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\results, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\train... found 1325 images in 5 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\val... found 332 images in 5 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    336645  ultralytics.nn.modules.head.Classify         [256, 5]                      \n",
      "YOLO11n-cls summary: 86 layers, 1,537,509 parameters, 1,537,509 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 13.32.5 MB/s, size: 91.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\train... 1325 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1325/1325 [00:01<00:00, 860.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 11.96.6 MB/s, size: 158.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\val... 332 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 332/332 [00:00<00:00, 442.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\\fold_1\\results\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100       1.6G      1.112          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166/166 [00:12<00:00, 13.30it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:01<00:00, 20.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.774          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      1.79G     0.5468          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166/166 [00:08<00:00, 18.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:00<00:00, 25.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.855          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      1.79G     0.4989          8        640:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 131/166 [00:07<00:01, 17.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFOLDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(MODEL_FILE)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassify\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMGSZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete. Best weights in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov-env\\lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov-env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov-env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:420\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[0;32m    418\u001b[0m     loss_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    419\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m--> 420\u001b[0m         \u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%11.4g\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.3g\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (GB) GPU memory util\u001b[39;49;00m\n\u001b[0;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# losses\u001b[39;49;00m\n\u001b[0;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size, i.e. 8\u001b[39;49;00m\n\u001b[0;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# imgsz, i.e 640\u001b[39;49;00m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m     )\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mplots \u001b[38;5;129;01mand\u001b[39;00m ni \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_idx:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##TRAINS THE MODEL KFOLD\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_DIR    = r\"C:\\Users\\jazzb\\ImageDetection-Yolov11\\Road_Damage\\folds\"  # root path where fold_1, ‚Ä¶, fold_5 live\n",
    "MODEL_FILE  = r\"models\\yolo11n-cls.pt\"                         # pretrained classification model\n",
    "FOLDS       = 5\n",
    "EPOCHS      = 100\n",
    "IMGSZ       = 640\n",
    "BATCH       = 8                                        # reduced for lower VRAM usage\n",
    "PATIENCE    = 5\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MEM_FRACTION = 0.6                                     # 60% GPU memory cap\n",
    "# ----------------------------------------\n",
    "\n",
    "print(f\"Using device: {DEVICE}\\n\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.set_per_process_memory_fraction(MEM_FRACTION, device=0)\n",
    "\n",
    "for fold in range(1, FOLDS + 1):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fold_dir = os.path.join(BASE_DIR, f\"fold_{fold}\")\n",
    "    if not os.path.isdir(os.path.join(fold_dir, 'train')):\n",
    "        raise FileNotFoundError(f\"{fold_dir}/train not found\")\n",
    "\n",
    "    print(f\"\\n=== Training Fold {fold}/{FOLDS} on folder {fold_dir} ===\")\n",
    "    model = YOLO(MODEL_FILE)\n",
    "    model.train(\n",
    "        data=fold_dir,\n",
    "        task='classify',\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMGSZ,\n",
    "        batch=BATCH,\n",
    "        patience=PATIENCE,\n",
    "        device=DEVICE,\n",
    "        project=fold_dir,\n",
    "        name='results',\n",
    "        exist_ok=True,\n",
    "        dropout=0.2,\n",
    "        weight_decay=0.0005,\n",
    "        amp=False\n",
    "    )\n",
    "\n",
    "    print(f\"--- Fold {fold} complete. Best weights in {fold_dir}/results ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c812c2e-f402-46d7-9cc5-4f9834cee9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SHOWS WHICH FOLD IS THE BEST\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Automatically get the base directory where the script is run\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Dynamically find the fold directories inside the base folder\n",
    "fold_dirs = [f\"fold_{i+1}\" for i in range(5)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold in fold_dirs:\n",
    "    csv_path = os.path.join(base_dir, fold, \"results\", \"results.csv\")\n",
    "    print(f\"Checking: {csv_path} -> Exists: {os.path.exists(csv_path)}\")  # Debug line\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"‚ùå Missing results.csv in {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"üîç Columns in {csv_path}:\\n{df.columns.tolist()}\")  # Debugging line\n",
    "\n",
    "    # Look for the accuracy column\n",
    "    accuracy_column = None\n",
    "    for col in df.columns:\n",
    "        if 'accuracy' in col.lower():\n",
    "            accuracy_column = col\n",
    "            print(f\"‚úÖ Found column: {col}\")\n",
    "            break\n",
    "\n",
    "    if accuracy_column is None:\n",
    "        print(f\"‚ö†Ô∏è 'accuracy' column not found in {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    best_row = df.loc[df[accuracy_column].idxmax()]\n",
    "    train_loss = best_row[\"train/loss\"]\n",
    "    val_loss = best_row[\"val/loss\"]\n",
    "    loss_gap = abs(train_loss - val_loss)\n",
    "\n",
    "    results.append({\n",
    "        \"Fold\": fold,\n",
    "        \"Epoch\": int(best_row[\"epoch\"]),\n",
    "        \"Accuracy\": best_row[accuracy_column],\n",
    "        \"Train Loss\": train_loss,\n",
    "        \"Validation Loss\": val_loss,\n",
    "        \"Loss Gap\": loss_gap,\n",
    "        \"Learning Rate\": best_row[\"lr/pg0\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"\\nüìä 5-Fold Cross-Validation Summary:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Normalize accuracy and loss gap for ranking\n",
    "    results_df[\"Accuracy Rank\"] = results_df[\"Accuracy\"].rank(ascending=False)\n",
    "    results_df[\"Loss Gap Rank\"] = results_df[\"Loss Gap\"].rank(ascending=True)\n",
    "\n",
    "    # Combined score: prioritize high accuracy + low overfitting\n",
    "    results_df[\"Overall Score\"] = results_df[\"Accuracy Rank\"] + results_df[\"Loss Gap Rank\"]\n",
    "\n",
    "    best_fold = results_df.loc[results_df[\"Overall Score\"].idxmin()]\n",
    "    print(f\"\\nüéØ Selected Best Generalizing Fold: {best_fold['Fold']} (Epoch {best_fold['Epoch']})\")\n",
    "    print(f\"   Accuracy: {best_fold['Accuracy']:.4f}, Loss Gap: {best_fold['Loss Gap']:.4f}\")\n",
    "\n",
    "    # Adjust the model path to match the correct directory structure\n",
    "    best_model_path = os.path.join(base_dir, best_fold[\"Fold\"], \"results\", \"weights\", \"best.pt\")\n",
    "    dest_dir = os.path.join(base_dir, \"final_model\")\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.copy(best_model_path, os.path.join(dest_dir, \"best_fold.pt\"))\n",
    "        print(f\"üì¶ Copied best.pt to: {os.path.join(dest_dir, 'best_fold.pt')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing best.pt at: {best_model_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No valid results found. Please check the folder structure and CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b756c-5b8b-46ca-b2fe-cf5a2a1068f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURE CLASSIFICATION + NO_DAMAGE BY CONFIDENCE THRESHOLD\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Reload your trained classification weights\n",
    "weights = \"fold_1/results/weights/best.pt\"\n",
    "model   = YOLO(weights)\n",
    "\n",
    "# 2. Grab and extend the model's names dict\n",
    "names = model.names.copy()\n",
    "no_damage_idx = max(names.keys()) + 1\n",
    "names[no_damage_idx] = \"no_damage\"\n",
    "\n",
    "# 3. Validation setup\n",
    "val_root = \"Road_Damage/val\"\n",
    "folders  = [\n",
    "    \"potholes\",\n",
    "    \"crack_issues\",\n",
    "    \"alligator_crack_issues\",\n",
    "    \"ravelling\",\n",
    "    \"open_manhole\"\n",
    "]\n",
    "\n",
    "# 4. Invert for lookup\n",
    "name_to_idx = {v: k for k, v in names.items()}\n",
    "\n",
    "# 5. Choose a confidence threshold below which we call it \"no_damage\"\n",
    "CONF_THRESH = 0.3\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for folder in folders:\n",
    "    true_idx = name_to_idx[folder]\n",
    "    img_dir  = os.path.join(val_root, folder, \"images\")\n",
    "    for fn in os.listdir(img_dir):\n",
    "        if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(img_dir, fn)\n",
    "        y_true.append(true_idx)\n",
    "\n",
    "        # Run classification\n",
    "        res = model(path, imgsz=640)[0]\n",
    "\n",
    "        # Use top1 and top1conf directly\n",
    "        top_idx  = int(res.probs.top1)\n",
    "        top_conf = float(res.probs.top1conf)\n",
    "\n",
    "        # Assign no_damage if confidence is too low\n",
    "        if top_conf < CONF_THRESH:\n",
    "            pred_idx = no_damage_idx\n",
    "        else:\n",
    "            pred_idx = top_idx\n",
    "\n",
    "        y_pred.append(pred_idx)\n",
    "# 6. Print classification report (include labels for all classes)\n",
    "sorted_idx = sorted(name_to_idx.values())  # e.g. [0,1,2,3,4,5,6]\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=sorted_idx,\n",
    "    target_names=[names[i] for i in sorted_idx],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# 7. Build & save the confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=sorted_idx\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    cm,\n",
    "    display_labels=[names[i] for i in sorted_idx]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
    "plt.title(\"Validation Confusion Matrix (incl. no_damage)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = \"confusion_matrix_with_no_damage.png\"\n",
    "plt.savefig(out_path, dpi=150)\n",
    "print(f\"Saved confusion matrix to {out_path}\")\n",
    "plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
