{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438c0ea4-b5fd-453b-9a55-158474c83a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting detection and classification...\n",
      "[INFO] Running detector.predict() with parameters from standalone test...\n",
      "\n",
      "image 1/1 C:\\Users\\jazzb\\ImageDetection-Yolov9\\test image\\crack.png: 448x640 1 crack_issues, 71.4ms\n",
      "Speed: 2.7ms preprocess, 71.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[DEBUG] detector.predict() returned 1 result object(s).\n",
      "[DEBUG] Number of boxes found by detector IN SCRIPT: 1\n",
      "[INFO] Detections found! Attempting to display them directly using results[0].plot()...\n",
      "[INFO] Detection complete (after immediate check). Found 1 object(s) to process.\n",
      "[INFO] Detection step (manual drawing) completed.\n",
      "[INFO] Hybrid step completed.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load fine-tuned weights for detection and classification\n",
    "detector = YOLO('runs/detect/train33/weights/best.pt')    # Detection model\n",
    "classifier = YOLO('runs/classify/train17/weights/best.pt') # Classification model\n",
    "\n",
    "def detect_and_classify(image_path):\n",
    "    print(\"[INFO] Starting detection and classification...\")\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"[ERROR] Failed to load image at path: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Perform object detection - MODIFIED TO MATCH YOUR STANDALONE TEST\n",
    "    print(\"[INFO] Running detector.predict() with parameters from standalone test...\")\n",
    "    results = detector.predict(\n",
    "        source=image_path,\n",
    "        augment=True,  # Match standalone test\n",
    "        conf=0.25,     # Match standalone test\n",
    "        iou=0.45,      # Match standalone test\n",
    "        verbose=True   # Add verbose output from YOLO for more clues\n",
    "    )\n",
    "\n",
    "    # --- IMMEDIATE VERIFICATION OF DETECTION ---\n",
    "    if results and len(results) > 0:\n",
    "        print(f\"[DEBUG] detector.predict() returned {len(results)} result object(s).\")\n",
    "        # results[0] contains the detections for the first image\n",
    "        detected_boxes_in_script = results[0].boxes\n",
    "        print(f\"[DEBUG] Number of boxes found by detector IN SCRIPT: {len(detected_boxes_in_script)}\")\n",
    "\n",
    "        if len(detected_boxes_in_script) > 0:\n",
    "            print(\"[INFO] Detections found! Attempting to display them directly using results[0].plot()...\")\n",
    "            # .plot() returns the image with detections drawn on it\n",
    "            annotated_image_from_plot = results[0].plot()\n",
    "            cv2.imshow(\"Direct Detector Output (from script)\", annotated_image_from_plot)\n",
    "            cv2.waitKey(0) # Important: wait for a key press to see this window\n",
    "            # You can choose to destroy this specific window if you want before proceeding\n",
    "            # cv2.destroyWindow(\"Direct Detector Output (from script)\")\n",
    "        else:\n",
    "            print(\"[WARNING] No boxes found by detector IN SCRIPT, even with matched parameters.\")\n",
    "    else:\n",
    "        print(\"[ERROR] detector.predict() returned None or an empty list in script.\")\n",
    "        return # Exit if no results from detector\n",
    "\n",
    "    # Proceed with your original script logic, using 'results' from above\n",
    "    boxes = results[0].boxes # This should now be the same 'detected_boxes_in_script'\n",
    "    print(f\"[INFO] Detection complete (after immediate check). Found {len(boxes)} object(s) to process.\")\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        print(\"[WARNING] No objects detected to process for classification/hybrid steps.\")\n",
    "        # If no boxes, you might want to show the original image or just skip cv2.imshow for later steps\n",
    "        cv2.imshow(\"Detection Step (No Detections)\", image) # Show original if no detections\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return\n",
    "\n",
    "    # Step 1: Display detection-only (Bounding boxes) - This uses your manual drawing\n",
    "    detection_image = image.copy()\n",
    "    for box in boxes: # Iterate through the boxes found\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cv2.rectangle(detection_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        if hasattr(box, 'cls') and box.cls is not None and detector.names:\n",
    "             det_class_name = detector.names[int(box.cls[0])]\n",
    "             det_conf = float(box.conf[0])\n",
    "             cv2.putText(detection_image, f\"{det_class_name} ({det_conf:.2f})\", (x1, y1 - 10),\n",
    "                         cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Detection Step (Manual Drawing)\", detection_image)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"[INFO] Detection step (manual drawing) completed.\")\n",
    "\n",
    "    # Hybrid Step (Detection + Classification)\n",
    "    hybrid_image = image.copy()\n",
    "    # ... (rest of your classification and hybrid display logic from the previous good version) ...\n",
    "    # Ensure you use the 'boxes' variable obtained from the verified 'results'\n",
    "    if not boxes:\n",
    "        print(\"[INFO] No boxes detected, skipping classification and hybrid display.\")\n",
    "    else:\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            roi = image[y1:y2, x1:x2]\n",
    "            if roi.size == 0:\n",
    "                print(f\"[WARNING] Empty ROI at index {i} with coords ({x1},{y1},{x2},{y2}). Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                class_results_list = classifier.predict(source=roi, imgsz=224, verbose=False)\n",
    "                if not class_results_list:\n",
    "                    print(f\"[WARNING] Classification for ROI {i} returned no results. Skipping.\")\n",
    "                    continue\n",
    "                class_result = class_results_list[0]\n",
    "                if not hasattr(class_result, 'probs') or class_result.probs is None:\n",
    "                    print(f\"[WARNING] Classification for ROI {i} returned no probabilities. Skipping.\")\n",
    "                    continue\n",
    "                top1_idx = int(class_result.probs.top1)\n",
    "                class_name = classifier.names[top1_idx]\n",
    "                confidence = float(class_result.probs.top1conf)\n",
    "                label = f\"{class_name} ({confidence:.2f})\"\n",
    "                cv2.rectangle(hybrid_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(hybrid_image, label, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Error during classification of ROI {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    cv2.imshow(\"Hybrid Step (Detection + Classification)\", hybrid_image)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"[INFO] Hybrid step completed.\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test the process with an image\n",
    "detect_and_classify('test image/crack.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4179811f-0dae-4868-b829-c7292990a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\jazzb\\ImageDetection-Yolov9\\test image\\crack.png: 448x640 1 crack_issues, 73.9ms\n",
      "Speed: 2.2ms preprocess, 73.9ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train33/weights/best.pt')\n",
    "results = model.predict(source='test image/crack.png', augment=True, conf=0.25, iou=0.45)\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7b4a9-4d45-44f0-a67c-3fe82f5d00d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
